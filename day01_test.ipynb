{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기본 연산 - 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.FloatTensor([[1,2],\n",
    "                       [3,4]])\n",
    "b = torch.FloatTensor([[2,2],\n",
    "                       [3,3]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요소별 산술 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 4.],\n",
       "        [6., 7.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#덧셈\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.,  0.],\n",
       "        [ 0.,  1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 뺄셈\n",
    "a - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 1.0000],\n",
       "        [1.0000, 1.3333]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 나눗셈\n",
    "a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  4.],\n",
       "        [27., 64.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 제곱연산\n",
    "\n",
    "a ** b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True],\n",
       "        [ True, False]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 논리 연산(==)\n",
    "\n",
    "a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False],\n",
       "        [False,  True]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 논리 연산 (!=)\n",
    "a != b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인플레이스 연산\n",
    "앞에서 수행한 연산들의 결과 텐서는 메모리에 새롭게 활당됩니다.\n",
    "빈메모리에 저장되고 텐서 결과 값이 출력되는겁니다.\n",
    "\n",
    "하지만 지금 부터는 설명할 인플레이스 연산은 같은 연산을 수행하지만 결과값이 기존 텐서에 저장되는 **차이점**이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[2., 2.],\n",
      "        [3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  4.],\n",
      "        [ 9., 12.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[2., 2.],\n",
      "        [3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "print(a.mul(b)) # 연산은 a * b 와 같은 연산입니다. \n",
    "#연산결과는 새로운 메모리에 활당됩니다. 따라서 a를 출력하면 그대로인것을 볼수있습니다.\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  4.],\n",
      "        [ 9., 12.]])\n",
      "tensor([[ 2.,  4.],\n",
      "        [ 9., 12.]])\n",
      "tensor([[2., 2.],\n",
      "        [3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# 인플레이스 연산들은 밑줄이 함수명 뒤에 붙어있는 것이 특징입니다.\n",
    "# 따라서 곱셈 함수의 인플레이스 연산 함수는 mul_()입니다.\n",
    "print(a.mul_(b)) \n",
    "print(a) # 결과 값이 a에 저장되는 것을 볼수있음\n",
    "print(b)\n",
    "\n",
    "# 새로운 공간에 만들어서 저장하는것이 아닌 기존의 공간에 저장하므로 효율적일것같지만\n",
    "# 파이토치 측에서는 컬렉터가 호율적으로 작동하기에 굳이 인플레이스 연산을 사용할 필요는 없다고함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 차원 축소 연산 : 합과 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor([[1,2],\n",
    "                       [3,4]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.)\n"
     ]
    }
   ],
   "source": [
    "print(x.sum())\n",
    "# sum() 통해 합을 구할수있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5000)\n"
     ]
    }
   ],
   "source": [
    "print(x.mean())\n",
    "# mean()통해 평균값을 구할수있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "행렬의 전체 합이나 평균은 텐서나 행렬이 아닌 스칼라 값으로 저장되므로 차원이 축소된다고 볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 6.])\n",
      "tensor([3., 7.])\n"
     ]
    }
   ],
   "source": [
    "# 여기 함수에 dim() 인자에 원하는 연산의 차원을 넣어줄수 있다.\n",
    "# 간단하게 dim() 인자의 값은 없어지는 차워이라고 생각하면 쉽다.\n",
    "\n",
    "print(x.sum(dim=0))\n",
    "# dim()인자는 -1로도 줄수있는데 뒤에서 첫번째 값을 의미한다.\n",
    "print(x.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 브로드캐스트 연산\n",
    "\n",
    "### 탠서 + 스칼라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 3.],\n",
      "        [4., 5.]])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1,2],\n",
    "                       [3,4]])\n",
    "y = 1\n",
    "z = x + y\n",
    "print(z)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서 + 백터\n",
    "- 차원 수만 맞춰 주면 계산이 가능하다 예를 들어 3차원 과 1차원 의 계산일 경우 \n",
    "- 1차원을 3차원으로 강제로 늘려준다 그리고 계산을 한다 차원 만 맞으면 계산이 가능하니까 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.Size([2])\n",
      "tensor([[4., 7.],\n",
      "        [6., 9.]])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1,2],\n",
    "                       [3,4]])\n",
    "y = torch.FloatTensor([3,\n",
    "                       5])\n",
    "\n",
    "print(x.size())\n",
    "print(y.size())\n",
    "\n",
    "z = x + y\n",
    "print(z)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 2])\n",
      "torch.Size([2])\n",
      "tensor([[[4., 7.]]])\n",
      "torch.Size([1, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[[1,2]]])\n",
    "y = torch.FloatTensor([3,\n",
    "                       5])\n",
    "\n",
    "print(x.size())\n",
    "print(y.size())\n",
    "\n",
    "z = x + y\n",
    "print(z)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서 + 텐서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "torch.Size([2, 1])\n",
      "tensor([[4., 5.],\n",
      "        [6., 7.]])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1,2]])\n",
    "y = torch.FloatTensor([[3],\n",
    "                       [5]])\n",
    "\n",
    "print(x.size())\n",
    "print(y.size())\n",
    "\n",
    "z = x + y\n",
    "print(z)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서 형태 변환\n",
    "## View 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[[1,2],\n",
    "                        [3,4]],\n",
    "                       [[5,6],\n",
    "                        [7,8]],\n",
    "                       [[9,10],\n",
    "                        [11,12]]])\n",
    "\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.])\n",
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.]])\n",
      "tensor([[[ 1.,  2.,  3.,  4.]],\n",
      "\n",
      "        [[ 5.,  6.,  7.,  8.]],\n",
      "\n",
      "        [[ 9., 10., 11., 12.]]])\n"
     ]
    }
   ],
   "source": [
    "print(x.view(12))\n",
    "print(x.view(3,4))\n",
    "print(x.view(3,1,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.])\n",
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.]])\n",
      "tensor([[[ 1.,  2.,  3.]],\n",
      "\n",
      "        [[ 4.,  5.,  6.]],\n",
      "\n",
      "        [[ 7.,  8.,  9.]],\n",
      "\n",
      "        [[10., 11., 12.]]])\n"
     ]
    }
   ],
   "source": [
    "#만약 새로운 크기가 기존 텐서의 요소 개수와 맞지 않으면 오류가 발생하게 됩니다. \n",
    "#하지만, view함수에 -1을 활용하면 일일 이 요소 갯수를 맞추기 위해 노력할 필요 없음\n",
    "# -1이 들어간 차원의 크기는 다른 차원의 값들을 곱하고 남은 필요한 값이 자동으로 채워집니다.\n",
    "print(x.view(-1)) \n",
    "print(x.view(3,-1))\n",
    "print(x.view(-1,1,3))\n",
    "\n",
    "#결론 -1로 채우면 알아서 채워준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tgv99\\AppData\\Local\\Temp\\ipykernel_6664\\485452161.py:2: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  x.storage().data_ptr() == y.storage().data_ptr()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.view(3,4)\n",
    "x.storage().data_ptr() == y.storage().data_ptr()\n",
    "#view 함수의 결과 텐서 주소는 바뀌지 않는다. 따라서 다음 코드에서 y의 값을 변경할시\n",
    "#x값도 같이 변하게 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View 함수\n",
    "- view 함수는 메모리에 순차대로 선언된 텐서에서만 작동한다. 만약 이를 어길시 오류를 발생하게된다. \n",
    "- 오류가 발생시 contiguous 함수를 호출한뒤 view 함수를 호출하면 된다.\n",
    "\n",
    "##### coniguous 함수\n",
    "- coniguous함수를 사용하면 메모리에 새롭게 활당을 안한 상태로 결과값을 반환한다.\n",
    "\n",
    "##### reshape 함수\n",
    "- view함수와 동일하게 동작하되 congiuous 함수와 view함수를 순차적으로 후출한것과 동일하다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.]])\n"
     ]
    }
   ],
   "source": [
    "print(x.reshape(3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squeeze 함수\n",
    "- 함수 차원의 크기가 1인 모든 차원을 없애는 역활을 한다.\n",
    "- 매개 변수로 숫자가 들어갈수있으며, 숫자들은 인덱스 위치를 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[[1,2],\n",
    "                        [3,4]]])\n",
    "print(x.size())\n",
    "# Squeeze 함수는 차원의 크기가 1인차원을 없애주는 역활을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(x.squeeze())\n",
    "print(x.squeeze().size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.Size([1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# 매개변수로 숫자가 들어갈수있고 그숫자는 인덱스 위치를 나타낸다.\n",
    "print(x.squeeze(0).size())\n",
    "print(x.squeeze(1).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsqueeze 함수\n",
    "- Unsqueeze는 지정된 차원의 인덱스에 차원의 크기가 1인 차워을 삽입합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1,2],\n",
    "                       [3,4]])\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 2, 1])\n",
      "torch.Size([2, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x.unsqueeze(1).size())\n",
    "print(x.unsqueeze(-1).size())\n",
    "print(x.unsqueeze(2).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# reshape를 통해서 구할수있다.\n",
    "print(x.reshape(2,2,-1).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서 자르기 & 붙이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[[1,2],\n",
    "                        [3,4]],\n",
    "                       [[5,6],\n",
    "                        [7,8]],\n",
    "                       [[9,10],\n",
    "                        [11,12]]])\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "print(x[0])\n",
    "# 주의 할점은 첫번째 차원은 잘라내는 과정에서 사라졌다는것.\n",
    "# 즉 3*2*2이지만 2*2크기의 행렬만남음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9., 10.],\n",
      "        [11., 12.]])\n"
     ]
    }
   ],
   "source": [
    "# 음수를 넣어서 뒤에서 접근하는방법도 가능\n",
    "print(x[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.],\n",
      "        [ 5.,  6.],\n",
      "        [ 9., 10.]])\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 차원이 아닌 중간차원에 대해서 작업을 수행하고 싶을 경우콜론(:)기호를 사용하면 된다.\n",
    "# 배열에서 \n",
    "print(x[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(x[1:2,1:,:].size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split 함수\n",
    "- Split 함수는 텐서를 특정 차원에 대해서 원하는 크기로 잘라줍니다. \n",
    "- 다음 코드는 Split함수를 통해 첫 번째 차워의 크기가 4가 되도록 텐서를 등분한 후에 각각의 등분된 텐서 크기를 출력하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor(10,4)\n",
    "splits = x.split(4,dim=0)\n",
    "for s in splits:\n",
    "    print(s.size())\n",
    "\n",
    "# 10개를 4개씩 나눌경우 나머지 2개된다 그래서 마지막에는 크키가 [2,4]로 출력된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk함수  \n",
    "- split 함수는 개수에 상관 없이 원하는 크기로 나누었다면 크기에 상관없이 원하는 개수로 나누는 chunk 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "#3등분 하는코드\n",
    "x = torch.FloatTensor(8,4)\n",
    "chunks = x.chunk(3,dim=0)\n",
    "for c in chunks:\n",
    "    print(c.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Select 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[[1,1],\n",
    "                        [2,2]],\n",
    "                       [[3,3],\n",
    "                        [4,4]],\n",
    "                       [[5,5],\n",
    "                        [6,6]]])\n",
    "indice = torch.LongTensor([2,1])\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[5., 5.],\n",
      "         [6., 6.]],\n",
      "\n",
      "        [[3., 3.],\n",
      "         [4., 4.]]])\n",
      "torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "y = x.index_select(dim = 0,index = indice)\n",
    "print(y)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate 함수\n",
    "- 텐서를 서로 합쳐서 하나의 텐서로 만드는 함수\n",
    "- 하나로 합칠때 배열내의 두개 이상의 텐서를 합쳐서 하나로 변환\n",
    "- 합쳐질때 다른차원의 크기가 같아야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3]) torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]])\n",
    "y = torch.FloatTensor([[10,11,12],\n",
    "                       [13,14,15],\n",
    "                       [16,17,18]])\n",
    "\n",
    "print(x.size(),y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.],\n",
      "        [13., 14., 15.],\n",
      "        [16., 17., 18.]])\n",
      "torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "# 하나로 합치는함수\n",
    "# dim = 0 은 세로로 두텐서를 붙라는 코드임\n",
    "\n",
    "z = torch.cat([x,y],dim=0)\n",
    "print(z)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3., 10., 11., 12.],\n",
      "        [ 4.,  5.,  6., 13., 14., 15.],\n",
      "        [ 7.,  8.,  9., 16., 17., 18.]])\n",
      "torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "# dim =-1 로 줄경우 가로로 붙는다\n",
    "z = torch.cat([x,y],dim=-1)\n",
    "print(z)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3., 10., 11., 12.],\n",
      "        [ 4.,  5.,  6., 13., 14., 15.],\n",
      "        [ 7.,  8.,  9., 16., 17., 18.]])\n",
      "torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "z = torch.cat([x,y],dim=1)\n",
    "print(z)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3차원 텐서의 경우\n",
    "### 새로운 3차원 x 텐서 정의\n",
    "x_new = torch.FloatTensor([[[1, 2, 3], [4, 5, 6]]])  # Size: [1, 2, 3]\n",
    "### 새로운 3차원 y 텐서 정의\n",
    "y_new = torch.FloatTensor([[[7, 8, 9], [10, 11, 12]]])  # Size: [1, 2, 3]\n",
    "\n",
    "### dim=0으로 연결한 경우\n",
    "```\n",
    "tensor([[[ 1.,  2.,  3.],\n",
    "         [ 4.,  5.,  6.]],\n",
    "\n",
    "        [[ 7.,  8.,  9.],\n",
    "         [10., 11., 12.]]])\n",
    "Size: [2, 2, 3]\n",
    "```\n",
    "### dim=1으로 연결한 경우:\n",
    "```\n",
    "tensor([[[ 1.,  2.,  3.],\n",
    "         [ 4.,  5.,  6.],\n",
    "         [ 7.,  8.,  9.],\n",
    "         [10., 11., 12.]]])\n",
    "Size: [1, 4, 3]\n",
    "```\n",
    "### dim=2으로 연결한 경우:\n",
    "```\n",
    "tensor([[[ 1.,  2.,  3.,  7.,  8.,  9.],\n",
    "         [ 4.,  5.,  6., 10., 11., 12.]]])\n",
    "Size: [1, 2, 6]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stack 함수\n",
    "- 붙이는 작업이 아닌 쌓기 작업을 수행\n",
    "- cat 과 달리 사이즈가 달라도 상관없음\n",
    "- stack 함수는 새로운 차워을 만든뒤에 cat 함수를 수행한것과 동일하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x 텐서:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "x 텐서:\n",
      "torch.Size([3, 3])\n",
      "y 텐서:\n",
      "tensor([[10., 11., 12.],\n",
      "        [13., 14., 15.],\n",
      "        [16., 17., 18.]])\n",
      "y 텐서:\n",
      "torch.Size([3, 3])\n",
      "tensor([[[ 1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.],\n",
      "         [ 7.,  8.,  9.]],\n",
      "\n",
      "        [[10., 11., 12.],\n",
      "         [13., 14., 15.],\n",
      "         [16., 17., 18.]]])\n",
      "torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(f\"x 텐서:\\n{x}\")\n",
    "print(f\"x 텐서:\\n{x.size()}\")\n",
    "print(f\"y 텐서:\\n{y}\")\n",
    "print(f\"y 텐서:\\n{y.size()}\")\n",
    "z = torch.stack([x,y])\n",
    "print(z)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1., 10.],\n",
      "         [ 2., 11.],\n",
      "         [ 3., 12.]],\n",
      "\n",
      "        [[ 4., 13.],\n",
      "         [ 5., 14.],\n",
      "         [ 6., 15.]],\n",
      "\n",
      "        [[ 7., 16.],\n",
      "         [ 8., 17.],\n",
      "         [ 9., 18.]]])\n",
      "torch.Size([3, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "z = torch.stack([x,y],dim = -1)\n",
    "print(z)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3차원 텐서의 경우\n",
    "### 새로운 3차원 x 텐서 정의\n",
    "x_stack = torch.FloatTensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])  # Size: [2, 2, 2]\n",
    "### 새로운 3차원 y 텐서 정의\n",
    "y_stack = torch.FloatTensor([[[9, 10, 11], [12, 13, 14]]])  # Size: [1, 2, 3]\n",
    "\n",
    "### dim=0으로 연결한 경우:\n",
    "```\n",
    " tensor([[[[ 1.,  2.],\n",
    "          [ 3.,  4.]],\n",
    "\n",
    "         [[ 5.,  6.],\n",
    "          [ 7.,  8.]]],\n",
    "\n",
    "\n",
    "        [[[ 9., 10., 11.],\n",
    "          [12., 13., 14.]],\n",
    "\n",
    "         [[ 9., 10., 11.],\n",
    "          [12., 13., 14.]]]])\n",
    "Size of stacked z with dim=0: torch.Size([2, 2, 2, 3])\n",
    "``` \n",
    "### dim=1으로 연결한 경우:\n",
    "```\n",
    " tensor([[[[ 1.,  2.],\n",
    "          [ 3.,  4.]],\n",
    "\n",
    "         [[ 9., 10., 11.],\n",
    "          [12., 13., 14.]]],\n",
    "\n",
    "\n",
    "        [[[ 5.,  6.],\n",
    "          [ 7.,  8.]],\n",
    "\n",
    "         [[ 9., 10., 11.],\n",
    "          [12., 13., 14.]]]])\n",
    "Size of stacked z with dim=1: torch.Size([2, 2, 2, 3])\n",
    "\n",
    "```\n",
    "### dim=2으로 연결한 경우:\n",
    "```\n",
    " tensor([[[[ 1.,  9., 10., 11.],\n",
    "          [ 2.,  9., 10., 11.]],\n",
    "\n",
    "         [[ 3., 12., 13., 14.],\n",
    "          [ 4., 12., 13., 14.]]],\n",
    "\n",
    "\n",
    "        [[[ 5.,  9., 10., 11.],\n",
    "          [ 6.,  9., 10., 11.]],\n",
    "\n",
    "         [[ 7., 12., 13., 14.],\n",
    "          [ 8., 12., 13., 14.]]]])\n",
    "Size of stacked z with dim=2: torch.Size([2, 2, 2, 4])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stack 함수대신 unsqueeze 함수와 cat 함수로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x 텐서:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "x 텐서:\n",
      "torch.Size([3, 3])\n",
      "y 텐서:\n",
      "tensor([[10., 11., 12.],\n",
      "        [13., 14., 15.],\n",
      "        [16., 17., 18.]])\n",
      "y 텐서:\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(f\"x 텐서:\\n{x}\")\n",
    "print(f\"x 텐서:\\n{x.size()}\")\n",
    "print(f\"y 텐서:\\n{y}\")\n",
    "print(f\"y 텐서:\\n{y.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.],\n",
      "         [ 7.,  8.,  9.]],\n",
      "\n",
      "        [[10., 11., 12.],\n",
      "         [13., 14., 15.],\n",
      "         [16., 17., 18.]]])\n",
      "torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "d= 0\n",
    "#z = torch.stack([x,y],dim = d)\n",
    "z = torch.cat([x.unsqueeze(d),y.unsqueeze(d)],dim=0)\n",
    "\n",
    "print(z)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유용한 팁\n",
    "- cat 함수나 stack 함수는 실전에서 매우 유용하게 활용될떄가 많음,\n",
    "- 특히 여러이터레이션을 돌며 반복되는 작업을 수행한후 반복 작업의 결과물을 하나로 합쳐지는데 사용된다.\n",
    "(아마 GPU로 여러 계산을 병렬로 실행후 하나의 결과값으로 만드는과정중에 사용되나봄)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1.5000e+01, 2.3694e-38],\n",
      "        [2.3694e-38, 3.0000e+00]]), tensor([[1.4000e+01, 4.4766e+00],\n",
      "        [2.3694e-38, 9.0000e+00]]), tensor([[1.3000e+01, 2.7812e+00],\n",
      "        [2.3694e-38, 7.0000e+00]]), tensor([[-3.3006e+12,  1.4237e-42],\n",
      "        [ 1.1000e+01,  1.2000e+01]]), tensor([[11.0000,  1.8750],\n",
      "        [ 3.0000,  4.0000]])]\n",
      "tensor([[[ 1.5000e+01,  2.3694e-38],\n",
      "         [ 2.3694e-38,  3.0000e+00]],\n",
      "\n",
      "        [[ 1.4000e+01,  4.4766e+00],\n",
      "         [ 2.3694e-38,  9.0000e+00]],\n",
      "\n",
      "        [[ 1.3000e+01,  2.7812e+00],\n",
      "         [ 2.3694e-38,  7.0000e+00]],\n",
      "\n",
      "        [[-3.3006e+12,  1.4237e-42],\n",
      "         [ 1.1000e+01,  1.2000e+01]],\n",
      "\n",
      "        [[ 1.1000e+01,  1.8750e+00],\n",
      "         [ 3.0000e+00,  4.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "\n",
    "for i in range(5):\n",
    "    x = torch.FloatTensor(2,2)\n",
    "    result += [x]\n",
    "print(result)\n",
    "    \n",
    "result = torch.stack(result)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 유용한 함수들\n",
    "## Expand 함수\n",
    "- expand 함수는 차원의 크기가 1인 차원을 원하는 크기로 늘려주는 역활을 수행합니다. \n",
    "- 동일한 텐서를 그냥 반복하여 리스트에 넣고 cat함수를 해당차원에 대해 수행하는 것과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[[1,2]],\n",
    "                       [[3,4]]])\n",
    "print(x.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2.],\n",
      "         [1., 2.],\n",
      "         [1., 2.]],\n",
      "\n",
      "        [[3., 4.],\n",
      "         [3., 4.],\n",
      "         [3., 4.]]])\n",
      "torch.Size([2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "y = x.expand(2,3,2)\n",
    "print(y)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2.],\n",
      "         [1., 2.],\n",
      "         [1., 2.]],\n",
      "\n",
      "        [[3., 4.],\n",
      "         [3., 4.],\n",
      "         [3., 4.]]])\n",
      "torch.Size([2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# 만약 cat 함수로 구현할시\n",
    "y = torch.cat([x]*3,dim = 1)\n",
    "print(y)\n",
    "print(y.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Permutation 함수\n",
    "- rabdperm함수는 랜덤 수열을 생성하는 파이토치 함수이다.\n",
    "- 딥러닝은 랜덤성을 의존하는 부분이 많기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 7, 8, 6, 0, 3, 9, 5, 4, 1])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randperm(10)\n",
    "print(x)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argument Max 함수\n",
    "- argument 함수는 예를 들어설명하겠다\n",
    "- 다음 수식에서 argument가 의미하는 것은 set X 에서 뽑을수있는 x값중 출력값을 최대로 만드는 입력을 반환하는 함수\n",
    "- 즉 함수 f의 최대값이 아닌 f를 최대값으로 만드는 x를 반환하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[14,  9, 23],\n",
      "         [22, 20,  4],\n",
      "         [ 2, 17, 19]],\n",
      "\n",
      "        [[11, 12,  8],\n",
      "         [18, 25, 10],\n",
      "         [ 6,  0, 21]],\n",
      "\n",
      "        [[15, 13,  5],\n",
      "         [16, 24,  1],\n",
      "         [26,  3,  7]]])\n",
      "torch.Size([3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randperm(3**3).reshape(3,3,-1)\n",
    "print(x)\n",
    "print(x.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 0, 2],\n",
      "        [1, 1, 2],\n",
      "        [0, 1, 0]])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "y = x.argmax(dim = 2)\n",
    "print(y)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top-k 함수\n",
    "- argmax의 상위 호환 버전임\n",
    "- argmax 함수는 가장큰 한개의 값의 인덱스를 반환\n",
    "- topk 함수는 가장 큰 k개의 값과 인덱스를 모두 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "values, indice = torch.topk(x, k=1, dim = -1)\n",
    "print(values.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(indice.size())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort 함수\n",
    "- 텐서 x를 원하는 차원 기준으로 정렬한 후 k개를 뽑는것 \n",
    "- 즉, 결과물은 topk함수와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "_, indice = torch.topk(x,k = 2,dim=-1)\n",
    "print(indice.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[23, 14,  9],\n",
      "         [22, 20,  4],\n",
      "         [19, 17,  2]],\n",
      "\n",
      "        [[12, 11,  8],\n",
      "         [25, 18, 10],\n",
      "         [21,  6,  0]],\n",
      "\n",
      "        [[15, 13,  5],\n",
      "         [24, 16,  1],\n",
      "         [26,  7,  3]]])\n"
     ]
    }
   ],
   "source": [
    "target_dim = -1\n",
    "values, indice = torch.topk(x, k =x.size((target_dim)),largest=True)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked File 함수\n",
    "- 텐서내의 원하는 부분만 특정 값으로 바꿔치기 하는 함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.],\n",
      "        [6., 7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([i for i in range(3**2)]).reshape(3,-1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False],\n",
      "        [False, False,  True],\n",
      "        [ True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "#논리 연산자를 통해 불리언 텐서를 만들어보자\n",
    "mask = x > 4\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4., -1.],\n",
      "        [-1., -1., -1.]])\n"
     ]
    }
   ],
   "source": [
    "# 이마스크를 통해서 mask_file 함수를 수행한다면 4보다 큰 값을 갖는 요소들을 특정 값으로 치완할수있게 됩니다.\n",
    "# 다음 코드는 ㄱ보다 큰값을 모두 -1로 한번에 치환하도록 하는 코드입니다\n",
    "\n",
    "y = x.masked_fill(mask,value=-1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ones & Zeros 함수\n",
    "- 상수 값으로 텐서를 만들때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.ones(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.zeros(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1,2,3],\n",
    "                       [4,5,6]])\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 또는 ones_like와 zeros_like 함수를 통해서 특정 텐서와 같은 크기의 0또는 1 텐서를 만들수 있음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.ones_like(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.zeros_like(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
